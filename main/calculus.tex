\section{Multivariate calculus}
    In Apostol \cite{Apostol1969}, chapter 8, we can find a very nice mathematical summary of multivariate differential calculus. We'll start with multivariate functions, and subsequently discuss multivariate vector-valued functions.

    \subsection{Elementary topology}
        An open $n$-ball $\mathcal{B}(\vb{a}, \vb{r})$ is the set
        \begin{equation}
            \mathcal{B}(\vb{a}, \vb{r}) = \set{\vb{x} \in \R^n ; \thinspace \thinspace||\vb{x} - \vb{a}|| < \vb{r}} \thinspace .
        \end{equation}
        A point $\vb{a}$ is called an interior point of $S \subset \R^n$ if there exists an open $n$-ball such that \mbox{$\mathcal{B}(\vb{a}, \vb{r}) \subset S$}. \\

        A set $S \subseteq \R^n$ is called open if every point inside it is an interior point of $S$. Examples are in 1D an open interval, and in 3D the sphere without boundary. \\

        A neighborhood of a point $\vb{a}$ is an open set $S$ in which $\vb{a}$ lies. \\

        A point $\vb{x}$ is called an exterior point of $S \subset \R^n$ if there exists an open $n$-ball that does not contain any points of $S$. \\

        A point $\vb{b}$ is called a boundary point of $S$ if it is not interior nor exterior. The set of all boundary points of a set $S$ is called the boundary and is denoted by $\partial S$.

    \subsection{Limits and continuity}
        Let $S$ be an open subset of $\R^n$, and let $\vb{f}$ be a function
        \begin{equation}
            \vb{f}: S \rightarrow \R^m: \vb{x} \mapsto \vb{f}(\vb{x}) \thinspace .
        \end{equation}

        The limit notation, in which $\vb{x}$ approaches the interior point $\vb{a}$ has two equivalent meanings:
        \begin{equation}
            \lim_{\vb{x} \to \vb{a}} \vb{f}(\vb{x}) = \vb{b} \iff \lim_{ ||\vb{x} - \vb{a}|| \to \vb{0}} ||\vb{f}(\vb{x}) - \vb{b}|| = 0 \thinspace .
        \end{equation}
        The function $\vb{f}$ is called continuous at $\vb{a}$ if
        \begin{equation}
            \lim_{\vb{x} \to \vb{a}} \vb{f}(\vb{x}) = \vb{f}(\vb{a}) \thinspace .
        \end{equation}





        A partial derivative of $f$ along the $i$-th component is then defined as
        \begin{equation}
            \pdv{f(\vb{x})}{x_i} = \lim_{a \to 0} \qty( \frac{f(\vb{x} + a \vb{e}_i) - f(\vb{x})}{a} ) \thinspace .
        \end{equation}

        The gradient of $f$ evaluated at $\vb{x}$, then is
        \begin{equation}
            \grad{f(\vb{x})} \equiv \pdv{f(\vb{x})}{\vb{x}} \thinspace ,
        \end{equation}
        where the components are calculated as partial derivatives
        \begin{equation}
            \Big( \grad{f(\vb{x})} \Big)_i = \pdv{f(\vb{x})}{x_i} \thinspace .
        \end{equation}

        Furthermore, for the gradient of $f$ holds:
        \begin{equation}
            \lim_{h \to 0} \qty( \frac{f(\vb{x} + h \vb{a}) - f(\vb{x}) - h \grad{f(\vb{x})} \cdot \vb{a}}{h} ) = 0 \thinspace .
        \end{equation}

        The directional derivative of a function $f$ along a vector $\vb{a}$ is defined as
        \begin{equation}
            \grad_{\vb{a}} f(\vb{x}) = \lim_{h \to 0} \qty( \frac{f(\vb{x} + h \vb{a}) - f(\vb{x})}{h} )
        \end{equation}
        and can be calculated using
        \begin{equation}
            \grad_{\vb{a}} f(\vb{x}) = \grad{f(\vb{x})} \cdot \vb{a}
        \end{equation}
        for functions that are differentiable at $\vb{x}$. \\

        Some useful formulas concerning the derivative of scalar fields with respect to a vector are:
        \begin{align}
            & \pdv{\vb{x}} (\vb{x}^\text{T} \vb{y}) = \vb{y} \\
            & \pdv{\vb{x}} (\vb{x}^\text{T} \vb{x}) = 2 \vb{x} \\
            & \pdv{\vb{x}} (\vb{x}^\text{T} \vb{A} \vb{y}) = \vb{A} \vb{y} \\
            & \pdv{\vb{x}} (\vb{y}^\text{T} \vb{A} \vb{x}) = \vb{A}^\text{T} \vb{y} \\
            & \pdv{\vb{x}} (\vb{x}^\text{T} \vb{A} \vb{x}) = (\vb{A} + \vb{A}^\text{T}) \vb{x} \thinspace .
        \end{align}

        We can also calculate second-order (and subsequently higher-order) derivatives of the scalar function with respect to the components of $\vb{x}$. This second-order derivative is a symmetric matrix for twice-differentiable functions and is called the Hessian:
        \begin{equation}
            \vb{H}(\vb{x})_{ij} = \pdv{f(\vb{x})}{x_i}{x_j} \thinspace .
        \end{equation}

        Using the previously defined gradient and Hessian, we can write the Taylor expansion of the function $f$ around the point $\vb{x}_0$ as
        \begin{align}
            f(\vb{x}) &= f(\vb{x}_0) + (\vb{x} - \vb{x}_0)^\text{T} \thinspace \grad{f(\vb{x}_0)} + \frac{1}{2!} (\vb{x} - \vb{x}_0)^\text{T} \thinspace \vb{H}(\vb{x}) \thinspace (\vb{x} - \vb{x}_0) + \cdots \label{eq:taylor_matrix} \\
            &= f(\vb{x}_0) + \Delta \vb{x}^\text{T} \thinspace \grad{f(\vb{x}_0)} + \frac{1}{2!} \Delta \vb{x}^\text{T} \thinspace \vb{H}(\vb{x}_0) \thinspace \Delta \vb{x} + \cdots \thinspace ,
        \end{align}
        in which
        \begin{equation}
            \Delta \vb{x} = \vb{x} - \vb{x}_0 \thinspace .
        \end{equation}

        Equation (\ref{eq:taylor_matrix}) is actually just short-hand notation for the following:
        \begin{equation}
            f(\vb{x}) = f(\vb{x}_0) + \sum_i^n \eval{\pdv{f(\vb{x})}{x_i}}_{\vb{x}=\vb{x}_0} (x_i - x_{0,i}) + \frac{1}{2!} \sum_{ij}^n \eval{\pdv{f(\vb{x})}{x_i}{x_j}}_{\vb{x}=\vb{x}_0} (x_i - x_{0,i}) (x_j - x_{0,j}) + \cdots \thinspace .
        \end{equation}

        Often, we would like to separate the $n$ variables contained in $\vb{x}$ in say $m$ variables contained in $\vb{y}$ and $l$ variables contained in $\vb{z}$. Then, $f$ is the function
        \begin{equation}
            f: \R^m \cross \R^n \rightarrow \R: (\vb{y}, \vb{z}) \mapsto f(\vb{y}, \vb{z}) \thinspace .
        \end{equation}
        The gradient of $f$ is then a blocked vector:
        \begin{equation}
            \grad{f(\vb{y}, \vb{z})} =
            \begin{pmatrix}
                \pdv{f(\vb{y}, \vb{z})}{\vb{y}} \\
                \pdv{f(\vb{y}, \vb{z})}{\vb{z}}
            \end{pmatrix} =
            \begin{pmatrix}
                \grad_{\vb{y}}{f(\vb{y}, \vb{z})} \\
                \grad_{\vb{z}}{f(\vb{y}, \vb{z})}
            \end{pmatrix}
            \thinspace ,
        \end{equation}
        and the Hessian is a blocked matrix:
        \begin{equation}
            \vb{H}(\vb{y}, \vb{z}) =
            \begin{pmatrix}
                \pdv[2]{f(\vb{y}, \vb{z})}{\vb{y}} & \pdv{f(\vb{y}, \vb{z})}{\vb{y}}{\vb{z}} \\
                \pdv{f(\vb{y}, \vb{z})}{\vb{z}}{\vb{y}} & \pdv[2]{f(\vb{y}, \vb{z})}{\vb{z}}
            \end{pmatrix}
            =
            \begin{pmatrix}
                \vb{H}_{\vb{y} \vb{y}}(\vb{y}, \vb{z}) & \vb{H}_{\vb{y} \vb{z}}(\vb{y}, \vb{z}) \\
                \vb{H}_{\vb{z} \vb{y}}(\vb{y}, \vb{z}) & \vb{H}_{\vb{z} \vb{z}}(\vb{y}, \vb{z})
            \end{pmatrix} \thinspace .
        \end{equation}
        This means that an expression for the Taylor expansion of $f$ around $(\vb{y}_0, \vb{z}_0)$ becomes
        \begin{equation}
            \begin{split}
                f(\vb{y}, \vb{z}) = &f(\vb{y}_0, \vb{z}_0) + \Delta \vb{y}^\text{T} \thinspace \grad_{\vb{y}}{f(\vb{y}_0, \vb{z}_0)} + \Delta \vb{z}^\text{T} \thinspace \grad_{\vb{z}}{f(\vb{y}_0, \vb{z}_0)} \\
                &+ \frac{1}{2!} \Delta \vb{y}^\text{T} \thinspace \vb{H}_{\vb{y} \vb{y}}(\vb{y}_0, \vb{z}_0) \Delta \vb{y} + \frac{1}{2!} \Delta \vb{y}^\text{T} \thinspace \vb{H}_{\vb{y} \vb{z}}(\vb{y}_0, \vb{z}_0) \Delta \vb{z} \\
                &+ \frac{1}{2!} \Delta \vb{z}^\text{T} \thinspace \vb{H}_{\vb{z} \vb{y}}(\vb{y}_0, \vb{z}_0) \Delta \vb{y} + \frac{1}{2!} \Delta \vb{z}^\text{T} \thinspace \vb{H}_{\vb{z} \vb{z}}(\vb{y}_0, \vb{z}_0) \Delta \vb{z} + \cdots \thinspace .
            \end{split}
        \end{equation}

    \subsection{Vector fields - multivariate vector functions}
        Let $\vb{f}(\vb{x})$ be a vector-valued function, i.e. a vector field:
        \begin{equation}
            \vb{f}: \R^n \rightarrow \R^m: \vb{x} \mapsto \vb{f}(\vb{x}) \thinspace ,
        \end{equation}
        in which we can write
        \begin{align} \label{eq:vector_field}
            &\vb{f}(\vb{x}) = (f_1(\vb{x}), f_2(\vb{x}), \dots, f_m(\vb{x})) \\
            &\forall f_i: \R^n \rightarrow \R: \vb{x} \mapsto f_i(\vb{x}) \thinspace ,
        \end{align}
        in which the functions $f_i$ are sometimes called coordinate functions \cite{Burden2011}. In a sense, the vector field associates to every vector $\vb{x} \in \R^n$ a vector $\vb{f}(\vb{x}) \in \R^m$. \\

        The first-order derivative of a vector field is the matrix
        \begin{equation}
            \vb{J} \equiv \pdv{\vb{f}}{\vb{x}} \thinspace ,
        \end{equation}
        which is called the Jacobian and has entries
        \begin{equation} \label{eq:jacobian}
            \vb{J}(\vb{x})_{ij} = \pdv{f_i(\vb{x})}{x_j} \thinspace .
        \end{equation}

        Since the gradient of a scalar function is also a vector, we can take the Jacobian of this gradient, leading to
        \begin{equation}
            \pdv{\vb{x}} \bigg( \grad{f(\vb{x})} \bigg) = \vb{H}(\vb{x})^\text{T} \thinspace ,
        \end{equation}
        which means that the Jacobian of the gradient is the transpose of the Hessian. So, for twice differentiable functions the Hessian is equal to the Jacobian of the gradient. \\

        A useful formula concerning the derivative of vector fields with respect to a vector is
        \begin{align}
            \pdv{\vb{x}}{\vb{x}} = \vb{I} \thinspace .
        \end{align}
