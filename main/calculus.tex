\section{Multivariate calculus}
    In Apostol \cite{Apostol1969}, chapter 8, we can find a very nice mathematical summary of multivariate differential calculus. We'll start with multivariate functions, and subsequently discuss multivariate vector-valued functions.

    \subsection{Elementary topology}
        An open $n$-ball $\mathcal{B}(\vb{a}, \vb{r})$ is the set
        \begin{equation}
            \mathcal{B}(\vb{a}, \vb{r}) = \set{\vb{x} \in \R^n ; \thinspace \thinspace||\vb{x} - \vb{a}|| < \vb{r}} \thinspace .
        \end{equation}
        A point $\vb{a}$ is called an interior point of $S \subset \R^n$ if there exists an open $n$-ball such that \mbox{$\mathcal{B}(\vb{a}, \vb{r}) \subset S$}. \\

        A set $S \subseteq \R^n$ is called open if every point inside it is an interior point of $S$. Examples are in 1D an open interval, and in 3D the sphere without boundary. \\

        A neighborhood of a point $\vb{a}$ is an open set $S$ in which $\vb{a}$ lies. \\

        A point $\vb{x}$ is called an exterior point of $S \subset \R^n$ if there exists an open $n$-ball that does not contain any points of $S$. \\

        A point $\vb{b}$ is called a boundary point of $S$ if it is not interior nor exterior. The set of all boundary points of a set $S$ is called the boundary and is denoted by $\partial S$.

    \subsection{Limits and continuity for scalar functions}
        Let $S$ be an open subset of $\R^n$, and let $\vb{f}$ be a function
        \begin{equation}
            \vb{f}: S \rightarrow \R^m: \vb{x} \mapsto \vb{f}(\vb{x}) \thinspace ,
        \end{equation}
        in which we will designate vector-valued functions (i.e. $m \leq 1$) by a bold-face symbol. If we instead want to emphasize a scalar function (i.e. $m=1$), we will use an italic symbol. \\

        The limit notation, in which $\vb{x}$ approaches the interior point $\vb{a}$ has two equivalent meanings:
        \begin{equation}
            \lim_{\vb{x} \to \vb{a}} \vb{f}(\vb{x}) = \vb{b} \iff \lim_{ ||\vb{x} - \vb{a}|| \to \vb{0}} ||\vb{f}(\vb{x}) - \vb{b}|| = 0 \thinspace .
        \end{equation}
        The function $\vb{f}$ is called continuous at $\vb{a}$ if
        \begin{equation}
            \lim_{\vb{x} \to \vb{a}} \vb{f}(\vb{x}) = \vb{f}(\vb{a}) \thinspace .
        \end{equation}

    \subsection{Directional derivatives for scalar functions}
        Let $f$ be a scalar function:
        \begin{equation}
            \vb{f}: S \rightarrow \R: \vb{x} \mapsto f(\vb{x}) \thinspace ,
        \end{equation}
        in which $S$ is an open subset of $\R^n$ and $\vb{y}$ is a vector in $\R^n$. Let $\vb{a}$ be an interior point of $S$. We then call
        \begin{equation}
            \lim_{h \to 0} \qty( \frac{f(\vb{a} + h\vb{y}) - f(\vb{a})}{h} ) = f'(\vb{a};\vb{y})
        \end{equation}
        the derivative of $f$ with respect to $\vb{y}$ at $\vb{a}$. \\

        If
        \begin{equation}
            g(t) = f(\vb{a} + t \vb{y}) \thinspace ,
        \end{equation}
        then
        \begin{equation}
            g'(t) = f'(\vb{a} + t \vb{y}; \vb{y}) \thinspace .
        \end{equation}

        If $f(\vb{a} + t \vb{y})$ is differentiable for all $t \in \interval{0}{1}$, then by the mean value theorem we have:
        \begin{equation}
            \exists \theta \in \interval[open]{0}{1}: \qquad f(\vb{a} + \vb{y}) - f(\vb{a}) = f'(\vb{a} + \theta \vb{y}; \vb{y}) \thinspace .
        \end{equation}

        If $\vb{y}$ is a unit vector (i.e. $||\vb{y}|| = 1$), then we call $f'(\vb{a};\vb{y})$ a special name: the directional derivative of $f$ w.r.t. $\vb{y}$ in $\vb{a}$ and we assign a new symbol to it:
        \begin{equation}
            \grad_{\vb{y}} f(\vb{a}) = \lim_{h \to 0} \qty( \frac{f(\vb{a} + h\vb{y}) - f(\vb{a})}{h} ) = f'(\vb{a};\vb{y}) \thinspace ,
        \end{equation}
        which is equivalent with
        \begin{equation}
            \lim_{h \to 0} \qty( \frac{f(\vb{a} + h \vb{y}) - f(\vb{a}) - h \grad_{\vb{y}} f(\vb{a})}{h} ) = 0 \thinspace .
        \end{equation}
        The $i$-th partial derivative of $f$ in $\vb{a}$ is defined to be the directional derivative along $\vb{e}_i$. We define a new symbol
        \begin{equation}
            \pdv{f(\vb{a})}{x_i} = \grad_{\vb{e}_i} f(\vb{a}) = f'(\vb{a};\vb{e}_i) = \lim_{h \to 0} \qty( \frac{f(\vb{a} + h\vb{e}_i) - f(\vb{a})}{h} ) \thinspace .
        \end{equation}

    \subsection{The total derivative for scalar functions}
        Let $S$ be an open subset of $\R^n$ and $\vb{a}$, such that we define the scalar function $f$:
        \begin{equation}
            \vb{f}: S \rightarrow \R: \vb{x} \mapsto f(\vb{x}) \thinspace ,
        \end{equation}
        Let $\vb{a}$ be an interior point of $S$, and let's choose an $\vb{r}$ such that the open $n$-ball is contained by $S$:
        \begin{equation}
            \mathcal{B}(\vb{a},\vb{r}) \subseteq S \thinspace .
        \end{equation}
        Now let's choose a $\vb{v}$ with $||\vb{v}|| < ||\vb{r}||$ such that
        \begin{equation}
            \vb{a} + \vb{v} \in \mathcal{B}(\vb{a},\vb{r}) \thinspace .
        \end{equation}
        These are all the ingredients we need to define differentiability. We call the function $f$ differentiable at $\vb{a}$ if there exists a linear transformation $\vb{T}_{\vb{a}}$
        \begin{equation}
            \vb{T}_{\vb{a}}: \R^n \to \R
        \end{equation}
        such that $f$ admits a first-order Taylor formula:
        \begin{equation}
            f(\vb{a} + \vb{v}) = f(\vb{a}) + \vb{T}_{\vb{a}}(\vb{v}) + ||\vb{v}||Â E(\vb{a},\vb{\vb{v}}) \thinspace ,
        \end{equation}
        in which $E(\vb{a},\vb{\vb{v}})$ is a scalar function with the behaviour that $E(\vb{a},\vb{\vb{v}}) \to 0$ if $||\vb{v}|| \to 0$. We call this linear map the total derivative and we can equivalently write:
        \begin{equation}
            \lim_{h \to 0} \qty( \frac{f(\vb{a} + h \vb{y}) - f(\vb{a}) - h \vb{T}_{\vb{a}}(\vb{y})}{h} ) = 0 \thinspace .
        \end{equation}

        The total derivative is related to the directional derivative:
        \begin{equation}
            \vb{T}_{\vb{a}}(\vb{y}) = f'(\vb{a};\vb{y}) = \grad_{\vb{y}} f(\vb{a}) = \lim_{h \to 0} \qty( \frac{f(\vb{a} + h\vb{y}) - f(\vb{a})}{h} ) \thinspace ,
        \end{equation}
        and the following useful formula holds:
        \begin{equation}
            \vb{T}_{\vb{a}} (\vb{y}) = \grad{f(\vb{a})} \cdot \vb{y} \thinspace ,
        \end{equation}
        in which we have introduced the gradient of $f$ at the point $\vb{a}$. This is the vector of partial derivatives:
        \begin{equation}
            \Big( \grad{f(\vb{x})} \Big)_i \equiv \Big( \pdv{f}{\vb{x}} \Big)_i = \pdv{f(\vb{x})}{x_i} \thinspace .
        \end{equation}

        The directional derivative of a function $f$ along a vector $\vb{a}$ is defined as
        \begin{equation}
            \grad_{\vb{a}} f(\vb{x}) = \lim_{h \to 0} \qty( \frac{f(\vb{x} + h \vb{a}) - f(\vb{x})}{h} )
        \end{equation}
        and can be calculated using
        \begin{equation}
            \grad_{\vb{a}} f(\vb{x}) = \grad{f(\vb{x})} \cdot \vb{a}
        \end{equation}
        for functions that are differentiable at $\vb{x}$. \\

        Some useful formulas concerning the derivative of scalar fields with respect to a vector are:
        \begin{align}
            & \pdv{\vb{x}} (\vb{x} \cdot \vb{y}) = \vb{y} \\
            & \pdv{\vb{x}} (\vb{x} \cdot \vb{x}) = 2 \vb{x} \\
            & \pdv{\vb{x}} (\vb{x} \cdot (\vb{A} \vb{y})) = \vb{A} \vb{y} \\
            & \pdv{\vb{x}} (\vb{y} \cdot (\vb{A} \vb{x})) = \vb{A}^\text{T} \vb{y} \\
            & \pdv{\vb{x}} (\vb{x} \cdot (\vb{A} \vb{x})) = (\vb{A} + \vb{A}^\text{T}) \vb{x} \thinspace .
        \end{align}

    \subsection{The Hessian for scalar functions}
        We can also calculate second-order (and subsequently higher-order) derivatives of the scalar function with respect to the components of $\vb{x}$. This second-order derivative is a symmetric matrix for twice-differentiable functions and is called the Hessian:
        \begin{equation}
            \vb{H}(\vb{x})_{ij} = \pdv{f(\vb{x})}{x_i}{x_j} \thinspace .
        \end{equation}

        Using the previously defined gradient and Hessian, we can write the Taylor expansion of the function $f$ around the point $\vb{x}_0$ as
        \begin{align}
            f(\vb{x}) &= f(\vb{x}_0) + (\vb{x} - \vb{x}_0) \cdot \grad{f(\vb{x}_0)} + \frac{1}{2!} (\vb{x} - \vb{x}_0) \cdot  (\vb{H}(\vb{x}) \thinspace (\vb{x} - \vb{x}_0)) + \cdots \label{eq:taylor_matrix} \\
            &= f(\vb{x}_0) + \Delta \vb{x} \cdot \grad{f(\vb{x}_0)} + \frac{1}{2!} \Delta \vb{x}^\text{T} \cdot (\vb{H}(\vb{x}_0) \thinspace \Delta \vb{x}) + \cdots \thinspace ,
        \end{align}
        in which
        \begin{equation}
            \Delta \vb{x} = \vb{x} - \vb{x}_0 \thinspace .
        \end{equation}

        Equation (\ref{eq:taylor_matrix}) is actually just short-hand notation for the following:
        \begin{equation}
            f(\vb{x}) = f(\vb{x}_0) + \sum_i^n \eval{\pdv{f(\vb{x})}{x_i}}_{\vb{x}=\vb{x}_0} (x_i - x_{0,i}) + \frac{1}{2!} \sum_{ij}^n \eval{\pdv{f(\vb{x})}{x_i}{x_j}}_{\vb{x}=\vb{x}_0} (x_i - x_{0,i}) (x_j - x_{0,j}) + \cdots \thinspace .
        \end{equation}

        Often, we would like to separate the $n$ variables contained in $\vb{x}$ in say $m$ variables contained in $\vb{y}$ and $l$ variables contained in $\vb{z}$. Then, $f$ is the function
        \begin{equation}
            f: \R^m \cross \R^n \rightarrow \R: (\vb{y}, \vb{z}) \mapsto f(\vb{y}, \vb{z}) \thinspace .
        \end{equation}
        The gradient of $f$ is then a blocked vector:
        \begin{equation}
            \grad{f(\vb{y}, \vb{z})} =
            \begin{pmatrix}
                \pdv{f(\vb{y}, \vb{z})}{\vb{y}} \\
                \pdv{f(\vb{y}, \vb{z})}{\vb{z}}
            \end{pmatrix}
            \thinspace ,
        \end{equation}
        and the Hessian is a blocked matrix:
        \begin{equation}
            \vb{H}(\vb{y}, \vb{z}) =
            \begin{pmatrix}
                \pdv[2]{f(\vb{y}, \vb{z})}{\vb{y}} & \pdv{f(\vb{y}, \vb{z})}{\vb{y}}{\vb{z}} \\
                \pdv{f(\vb{y}, \vb{z})}{\vb{z}}{\vb{y}} & \pdv[2]{f(\vb{y}, \vb{z})}{\vb{z}}
            \end{pmatrix}
            =
            \begin{pmatrix}
                \vb{H}_{\vb{y} \vb{y}}(\vb{y}, \vb{z}) & \vb{H}_{\vb{y} \vb{z}}(\vb{y}, \vb{z}) \\
                \vb{H}_{\vb{z} \vb{y}}(\vb{y}, \vb{z}) & \vb{H}_{\vb{z} \vb{z}}(\vb{y}, \vb{z})
            \end{pmatrix} \thinspace .
        \end{equation}
        This means that an expression for the Taylor expansion of $f$ around $(\vb{y}_0, \vb{z}_0)$ becomes
        \begin{equation}
            \begin{split}
                f(\vb{y}, \vb{z}) = &f(\vb{y}_0, \vb{z}_0) + \Delta \vb{y} \cdot \pdv{\vb{y}} f(\vb{y}_0, \vb{z}_0) + \Delta \vb{z} \cdot \pdv{\vb{z}} f(\vb{y}_0, \vb{z}_0) \\
                &+ \frac{1}{2!} \Delta \vb{y} \cdot (\vb{H}_{\vb{y} \vb{y}}(\vb{y}_0, \vb{z}_0) \Delta \vb{y}) + \frac{1}{2!} \Delta \vb{y} \cdot (\vb{H}_{\vb{y} \vb{z}}(\vb{y}_0, \vb{z}_0) \Delta \vb{z}) \\
                &+ \frac{1}{2!} \Delta \vb{z} \cdot (\vb{H}_{\vb{z} \vb{y}}(\vb{y}_0, \vb{z}_0) \Delta \vb{y}) + \frac{1}{2!} \Delta \vb{z} \cdot (\vb{H}_{\vb{z} \vb{z}}(\vb{y}_0, \vb{z}_0) \Delta \vb{z}) + \cdots \thinspace .
            \end{split}
        \end{equation}

    \subsection{Vector fields - multivariate vector functions}
        Let $\vb{f}(\vb{x})$ be a vector-valued function, i.e. a vector field:
        \begin{equation}
            \vb{f}: S \rightarrow \R^m: \vb{x} \mapsto \vb{f}(\vb{x}) \thinspace ,
        \end{equation}
        in which $S$ is an open subset of $\R^n$. We will also write:
        \begin{align} \label{eq:vector_field}
            &\vb{f}(\vb{x}) = (f_1(\vb{x}), f_2(\vb{x}), \dots, f_m(\vb{x})) \\
            &\forall f_i: \R^n \rightarrow \R: \vb{x} \mapsto f_i(\vb{x}) \thinspace ,
        \end{align}
        in which the functions $f_i$ are sometimes called coordinate functions \cite{Burden2011}. In a sense, the vector field associates to every vector $\vb{x} \in \R^n$ a vector $\vb{f}(\vb{x}) \in \R^m$. Since this is a generaliation from $\R$ to $\R^n$, we can easily generalize the previous formulas for scalar functions to vector functions. \\

        We can now write
        \begin{equation}
            \vb{f}(\vb{a}; \vb{y}) = \lim_{h \to 0} \qty( \frac{\vb{f}(\vb{a} + h \vb{y}) - \vb{f}(\vb{a})}{h} ) \thinspace .
        \end{equation}
        The vector function $\vb{f}$ is now called differentiable at a point $\vb{a}$ is there exists a linear map called the total derivative of $f$ at $\vb{a}$
        \begin{equation}
            \vb{T}_{\vb{a}}: \R^n \to \R^n \thinspace ,
        \end{equation}
        such that $\vb{f}$ admits a Taylor formula:
        \begin{equation}
            \vb{f}(\vb{a} + \vb{v}) + \vb{T}_{\vb{a}}(\vb{v}) + ||\vb{v}|| \vb{E}(\vb{a}, \vb{v}) \thinspace ,
        \end{equation}
        in which $\vb{E}(\vb{a}, \vb{v}) \to \vb{0}$ as $||\vb{v}|| \to \vb{0}$. \\

        We can write this total derivative also as
        \begin{equation}
            \vb{T}_{\vb{a}} (\vb{y}) = \vb{f}'(\vb{a}, \vb{y}) = \sum_i^n \grad{f_i(\vb{a})} \cdot \vb{y} \vb{e}_i = \vb{J}(\vb{a}) \vb{y} \thinspace ,
        \end{equation}
        which leads to the conclusion that the matrix $\vb{J}$, which we will call the Jacobian matrix, is the matrix representation of the total derivative for vector fields. \\

        We can also say that first-order derivative of a vector field is the matrix
        \begin{equation}
            \vb{J} \equiv \pdv{\vb{f}}{\vb{x}} \thinspace ,
        \end{equation}
        which is called the Jacobian and has entries
        \begin{equation} \label{eq:jacobian}
            \vb{J}(\vb{x})_{ij} = \pdv{f_i(\vb{x})}{x_j} \thinspace .
        \end{equation}

        Since the gradient of a scalar function is also a vector, we can take the Jacobian of this gradient, leading to
        \begin{equation}
            \pdv{\vb{x}} \bigg( \grad{f(\vb{x})} \bigg) = \vb{H}(\vb{x})^\text{T} \thinspace ,
        \end{equation}
        which means that the Jacobian of the gradient is the transpose of the Hessian. So, for twice differentiable functions the Hessian is equal to the Jacobian of the gradient. \\

        A useful formula concerning the derivative of vector fields with respect to a vector is
        \begin{align}
            \pdv{\vb{x}}{\vb{x}} = \vb{I} \thinspace .
        \end{align}
