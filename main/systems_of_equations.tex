\section{Solving systems of equations}
    The goal of solving systems of equations is to solve equations of the type
    \begin{equation} \label{eq:system_of_equations}
        \vb{f}(\vb{x}) = \vb{0} \thinspace ,
    \end{equation}
    in which $\vb{f}$ is a vector field as in equation (\ref{eq:vector_field}). In order to solve equation (\ref{eq:system_of_equations}), we use a linear approximation of $\vb{f}$ at a specified $\vb{x}_0$:
    \begin{equation} \label{eq:system_of_equations_jacobian}
        \vb{f}(\vb{x}) \approx \vb{f}(\vb{x}_0) + \vb{J}(\vb{x}_0) (\vb{x} - \vb{x}_0) \thinspace ,
    \end{equation}
    in which $\vb{J}(\vb{x}_0)$ is the Jacobian (cfr. equation (\ref{eq:jacobian})) of $\vb{f}$, calculated at the point $\vb{x}_0$, which immediately signifies the meaning of the Jacobian: it is a generalization of the concept of derivative.

    \subsection{Newton's method}
        Combining equations (\ref{eq:system_of_equations}) and (\ref{eq:system_of_equations_jacobian}), we get
        \begin{align}
            \vb{f}(\vb{x}_0) + \vb{J}(\vb{x}_0) (\vb{x} - \vb{x}_0) &\approx \vb{0} \\
            \vb{J}(\vb{x}_0) (\vb{x} - \vb{x}_0) & \approx - \vb{f}(\vb{x}_0) \label{eq:system_of_equations_approx} \thinspace ,
        \end{align}
        which means that if we have an initial $\vb{x}_0$ (a guess for the solution), we can in principle calculate an improved guess $\vb{x}$. Newton's method thus goes as follows \cite{Burden2011}:
        \begin{enumerate}
            \item Choose an initial guess $\vb{x}_0$
            \item \label{en:newton:first} In the $(i+1)$-th iteration, solve (\ref{eq:system_of_equations_approx}) for $\Delta \vb{x}_{i+1} = \vb{x}_{i+1} - \vb{x}_i$.
            \item Update
                \begin{equation}
                    \vb{x}_{i+1} = \vb{x}_i + \Delta \vb{x}_{i+1}
                \end{equation}
            \item \label{en:newton:last} Recalculate the Jacobian and the vector field at $\vb{x}_{i+1}$.
            \item Repeat steps \ref{en:newton:first} to \ref{en:newton:last} until convergence is achieved:
                \begin{equation}
                    || \Delta \vb{x}_{i+1} || < \epsilon \thinspace .
                \end{equation}
        \end{enumerate}

    \subsection{Broyden's method}
        Broyden's method \cite{Burden2011, broyden1965} is a quasi-Newton method that eliminates the need to recompute the Jacobian matrix at every iteration step. In the $i+1$-th step of the iteration, we use an approximation for the Jacobian matrix, which we shall denote by $\vb{A}_i$:
        \begin{equation}
            \vb{A}_i (\vb{x}_{i+1} - \vb{x}_i) = \vb{f}(\vb{x}_{i+1}) - \vb{f}(\vb{x}_i) \thinspace ,
        \end{equation}
        or using a simplified notation:
        \begin{equation} \label{eq:broyden1}
            \vb{A}_i \Delta \vb{x}_{i+1} = \Delta \vb{f}_{i+1} \thinspace .
        \end{equation}
        These equations show the action of $\vb{A}_{i+1}$ on $\Delta \vb{x}_{i+1}$. However, to fully characterize $\vb{A}_{i+1}$, we must also know its action on a vector $\vb{z}_i$ that is in the orthogonal complement of $\Delta \vb{x}_{i+1}$. Since we have no information about this, we specify that there should be no change made in this direction, i.e.
        \begin{equation} \label{eq:broyden2}
            \forall \vb{z}_i: \vb{z}_i \cdot \Delta \vb{x}_{i} = 0: \qquad \vb{A}_{i+1} \vb{z}_i = \vb{A}_i \vb{z}_i
        \end{equation}
        We can show that the (unique) solution to equations (\ref{eq:broyden1}) and (\ref{eq:broyden2}) is
        \begin{equation}
            \vb{A}_{i+1} = \vb{A}_i + \frac{\Delta \vb{f}_{i+1} - \vb{A}_i \Delta \vb{x}_{i+1}}{|| \Delta \vb{x}_{i+1} ||^2} \Delta \vb{x}_{i+1}^T \thinspace ,
        \end{equation}
        which is of the form $(\vb{A} + \vb{x} \vb{y}^T)$, for $\vb{A}$ nonsingular and $\vb{y}^T \vb{A}^{-1} \vb{x} \neq -1$, such that we can use the Sherman-Morrison formula \cite{Burden2011}
        \begin{equation}
            (\vb{A} + \vb{x} \vb{y}^T)^{-1} = \vb{A}^{-1} - \frac{\vb{A}^{-1} \vb{x} \vb{y}^T \vb{A}^{-1}}{1 + \vb{y}^T \vb{A}^{-1} \vb{x}} \thinspace ,
        \end{equation}
        which leads to
        \begin{equation} \label{eq:broyden_A_update}
            \vb{A}_{i+1}^{-1} = \vb{A}_i^{-1} + \frac{(\Delta \vb{x}_{i+1} - \vb{A}^{-1}_i \Delta \vb{f}_{i+1}) \Delta \vb{x}_{i+1}^T \vb{A}_i^{-1}}{\Delta \vb{x}^T_{i+1} \vb{A}^{-1}_i \Delta \vb{f}_{i+1}} \thinspace .
        \end{equation}
        All in all, this makes the solution to equation (\ref{eq:broyden1}) when requiring $\vb{f}(\vb{x}_{i+1}) = \vb{0}$ easier to compute:
        \begin{equation} \label{eq:broyden_delta_x}
            \Delta \vb{x}_{i+1} = - \vb{A}_i^{-1} \vb{f}(\vb{x}_i) \thinspace ,
        \end{equation}
        which means that the (possibly) time-consuming step of calculating the Jacobian over and over again in Newton' method, is avoided. \\

        Broyden's method goes as follows:
        \begin{enumerate}
            \item Choose an initial guess $\vb{x}_0$ and calculate $\vb{A}_0^{-1} = \vb{J}(\vb{x}_0)^{-1}$, with $\vb{J}$ the exact Jacobian
            \item \label{en:broyden:first} Solve equation (\ref{eq:broyden_delta_x})
            \item Update the guess
                \begin{equation}
                    \vb{x}_{i+1} = \vb{x}_i + \Delta \vb{x}_{i+1}
                \end{equation}
            \item \label{en:broyden:last} Update the inverse of the approximate Jacobian matrix $\vb{A}_{i+1}$ through equation (\ref{eq:broyden_A_update})
            \item Repeat steps \ref{en:broyden:first} through \ref{en:broyden:last} until convergence is achieved:
                \begin{equation}
                    || \Delta \vb{x}_{i+1} || < \epsilon
                \end{equation}
        \end{enumerate}

    \subsection{DIIS}
        DIIS \cite{Pulay1980} (direct inversion of the iterative subspace) is a technique to accelerate convergence in solving linear equations. Let's say we have already found $n$ estimates to the problem $\set{\vb{x}_i}$. We can construct a new estimate $\vb{x}^*$ as a linear combination of the previous ones:
        \begin{equation} \label{eq:DIIS_linear_combination}
            \vb{x}^* = \sum_i^n c_i \vb{x}_i \thinspace .
        \end{equation}
        Let's say we also have a way to construct an error vector $\vb{e}_i$ for every estimate, such that our goal will be to minimize the error function in a least squares sense:
        \begin{equation}
            \norm{\sum_i^n c_i \vb{e}_i}^2 \thinspace ,
        \end{equation}
        under the constraint that
        \begin{equation}
            \sum_i^n c_i = 1 \thinspace .
        \end{equation}
        We can then construct the Lagriangian for this problem as
        \begin{equation}
            \mathcal{L}(\set{c_i}, \lambda) = \norm{\sum_i^n c_i \vb{e}_i}^2 - \lambda \qty( \sum_i^n c_i - 1 ) \thinspace ,
        \end{equation}
        and by introducing a suitable scalar product between the error vectors
        \begin{equation}
            B_{ij} = (\vb{e}_i, \vb{e}_j) \thinspace ,
        \end{equation}
        we can write the Lagrangian as
        \begin{equation}
            \mathcal{L}(\set{c_i}, \lambda) = \sum_{ij}^n c_i c_j B_{ij} - \lambda \qty(\sum_i^n c_i - 1) \thinspace .
        \end{equation}
        Differentiating with respect to $\lambda$ gives us back the original constraint, and differentiating with respect to every coefficient leads to
        \begin{equation}
            \pdv{\mathcal{L}}{c_k} = 0 = 2 \sum_i^n c_i B_{ik} - \lambda \thinspace ,
        \end{equation}
        as the inner product matrix is symmetric. Absorbing the factor 2 in the Lagrange multiplier, we find the following system of equations
        \begin{equation}
            \begin{pmatrix}
                B_{11}  & B_{12}    & \cdots    & B_{1n}    & -1      \\
                B_{21}  & B_{22}    & \cdots    & B_{2n}    & -1      \\
                \vdots  & \vdots    & \ddots    & \vdots    & \vdots  \\
                B_{n1}  & B_{n2}    & \cdots    & B_{nn}    & -1      \\
                -1      & -1        & \cdots    &  -1       & 0
            \end{pmatrix}
            \begin{pmatrix}
                c_1     \\
                c_2     \\
                \vdots  \\
                c_n     \\
                \lambda
            \end{pmatrix}
            =
            \begin{pmatrix}
                0       \\
                0       \\
                \vdots  \\
                0       \\
                -1
            \end{pmatrix} \thinspace .
        \end{equation}
        We can recognize a linear system of equations of the form $\vb{B}' \vb{y} = \vb{b}$, which can be easily solved using various decomposition techniques, leading to the coefficients $\qty{c_i}$ which are then used in constructing the new estimate as in equation (\ref{eq:DIIS_linear_combination}). \\

        What remains is to construct a way to define the error vectors associated to every estimate $\vb{x}_i$, and a standard way would be to define residual vectors as
        \begin{equation}
            \vb{e}_i = \vb{x}_{i+1} - \vb{x}_i \thinspace .
        \end{equation}
        As an example, after 3 standard iterations, we end up with a set $\qty{\vb{x}_1, \vb{x}_2, \vb{x}_3}$. We can then calculate
        \begin{align}
            &\vb{e}_1 = \vb{x}_2 - \vb{x}_1 \\
            &\vb{e}_2 = \vb{x}_3 - \vb{x}_2 \thinspace ,
        \end{align}
        We can then set up the DIIS equations to find the coefficients $c_1$ and $c_2$, in order to construct a new guess vector $\vb{x}_3$, which is in a sense 'overwritten'. This DIIS-improved guess vector is then used as a starting point for the next iteration.
