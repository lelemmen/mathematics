\section{The symmetric eigenvalue problem}
    In the following, we will be looking for the solutions of the symmetric eigenvalue problem \cite{Golub2013}:
    \begin{equation}
        \vb{A} \vb{x} = \lambda \vb{x} \thinspace ,
    \end{equation}
    with $\vb{A} \in \R^{n \times n}$ and $\vb{A}$ symmetric:
    \begin{equation}
        \vb{A}^\text{T} = \vb{A} \thinspace .
    \end{equation}

    \subsection{Ritz pairs} \label{sec:ritz}
        On a symmetric matrix $\vb{A}$, we can perform a \emph{symmetric Schur decomposition}. This states that there exists a real, orthogonal matrix $\vb{Q}$
        \begin{equation}
            \vb{Q} = \begin{pmatrix} \vb{q}_1 & \vb{q}_2 & \cdots & \vb{q}_n \end{pmatrix} \qquad \qquad \vb{Q}^\text{T} \vb{Q} = \vb{I}_{(n)}
        \end{equation}
        such that
        \begin{equation}
            \vb{Q}^\text{T} \vb{A} \vb{Q} = \diag(\lambda_1, \cdots, \lambda_n) \thinspace .
        \end{equation}
        Moreover,
        \begin{equation}
            \vb{A} \vb{q}_k = \lambda_k \vb{q}_k \thinspace .
        \end{equation}
        \\\
        Let $\vb{Q}_1 \in \R^{n \times r}$ (first $r$ rows of an orthogonal matrix) have independent columns $\vb{Q}_1^\text{T} \vb{Q}_1 = \vb{I}_{(r)}$. For some $\vb{S} \in \R^{r \times r}$, we call
        \begin{equation}
            \vb{A}\vb{Q}_1 - \vb{Q}_1 \vb{S}
        \end{equation}
        the \emph{residual matrix}. Then there exist $\mu_1, \dots, \mu_r \in \lambda(\vb{A})$:
        \begin{equation} \label{eq:mu_lambda_residue}
            \forall k=1, \dots, r: |Â \mu_k - \lambda_k(\vb{S})| \leq \sqrt{2} \thinspace || \vb{A}\vb{Q}_1 - \vb{Q}_1 \vb{S} ||_F \thinspace ,
        \end{equation}
        in which $\lambda_k(S)$ is the $k$-th largest eigenvalue of $\vb{S}$ and $||.||_F$ represents the matrix 2-norm (or Frobenius norm). This results says that the eigenvalues of $\vb{S}$ tend to the eigenvalues of $\vb{A}$, with the better approximation for the smaller Frobenius norm of the residual matrix.\\

        It is then natural to look at which $\vb{S}$ minimizes the right-hand side of equation (\ref{eq:mu_lambda_residue}). It can be shown that
        \begin{equation}
            \min_{\vb{S} \in \R^{r \times r}} || \vb{A}\vb{Q}_1 - \vb{Q}_1 \vb{S} ||_F = || (\vb{I}_{(n)} - \vb{Q}_1 \vb{Q}_1^\text{T}) \vb{A}\vb{Q}_1 ||_2 \thinspace ,
        \end{equation}
        with the minimizer being
        \begin{equation} \label{eq:residual_frobenius_minimizer}
            \vb{S} = \vb{Q}_1^\text{T} \vb{A} \vb{Q}_1 \thinspace .
        \end{equation}
        This means that we can associate any $r$-dimensional subspace $C(\vb{Q}_1)$ with a set of $r$ "optimal" eigenvalue-eigenvector approximates. Let us Schur-decompose the minimizer (equation (\ref{eq:residual_frobenius_minimizer})) to yield
        \begin{equation}
            \vb{Z}^\text{T} \vb{S} \vb{Z} = \diag(\theta_1, \dots, \theta_r) \thinspace ,
        \end{equation}
        and
        \begin{equation}
            \vb{Q}_1 \vb{Z} = \begin{pmatrix} \vb{y}_1 & \vb{y}_2 & \cdots & \vb{y}_r \end{pmatrix} \thinspace .
        \end{equation}
        The tuple $(\theta_k, \vb{y}_k)$ is then called a \emph{Ritz pair}, with $\theta_k$ being the \emph{Ritz value} and $\vb{y}_k$ begin the \emph{Ritz vector}, which are the optimal eigensystem approximates of the symmetric matrix $\vb{A}$ in the $r$-dimensional subspace $C(\vb{Q}_1)$.

    \subsection{Approximate eigenpairs and corrections}
        Let's say we have a current (subscript $c$) approximation $\qty{\lambda_c, \vb{x}_c}$ to the symmetric eigenvalue problem. Denoting the correction to the eigenvalue by $\delta\lambda$ and the correction to the eigenvector by $\boldsymbol{\delta} \vb{x}$, we then want
        \begin{equation} \label{eq:approximation_eigenequation}
            \vb{A} (\vb{x}_c + \boldsymbol{\delta} \vb{x}) = (\lambda_c + \delta\lambda)(\vb{x}_c + \boldsymbol{\delta} \vb{x})
        \end{equation}
        to hold. By introducing the current \emph{residual vector}
        \begin{equation}
            \vb{r}_c = \vb{A} \vb{x}_c - \lambda_c \vb{x}_c \thinspace ,
        \end{equation}
        we can rewrite equation (\ref{eq:approximation_eigenequation}) as
        \begin{equation} \label{eq:residual_vector_equation}
            \qty(\vb{A} - \lambda_c \vb{I}_{(n)}) \boldsymbol{\delta} \vb{x} - (\delta\lambda) \vb{x}_c = - \vb{r}_c \thinspace .
        \end{equation}
        Unfortunately, this is an underdetermined system of equations.

    \subsection{The Davidson diagonalization method}
        As proposed initially by Davidson in 1975 \cite{davidson1975}, his diagonalization method applied to any symmetry, diagonally dominant matrix of dimension $N$. It is a method to solve the associated eigenvalue problem for this matrix (which can have insanely large dimensions - large enough to be stored in the modern RAM-memory of a computer), finding its lowest eigenvalue and associated eigenvector. \\

        In summary, the algorithm takes an initial guess for the lowest-eigenvalue eigenvector and produces new estimates by solving the diagonalization in an ever increasing subspace of previous estimates. Starting from that initial guess, the algorithm goes as follows.
        \begin{enumerate}
            \item Let $\vb{t}$ be an initial guess vector. Calculate $\vb{v}_1$, being a new subspace vector, as
                \begin{equation}
                    \vb{v}_1 = \frac{\vb{t}}{|| \vb{t} ||} \thinspace .
                \end{equation}

            \item Calculate the (expensive) matrix-vector product:
                \begin{equation}
                    \vb{v}^{\vb{A}}_1 = \vb{A} \vb{v}_1 \thinspace .
                \end{equation}

            \item Expand the subspace $\vb{V}$ (initially it is empty) to
                \begin{equation}
                    \vb{V}_1 = \begin{pmatrix} \vb{v}_1 \end{pmatrix}
                \end{equation}
                and expand (again: initially $\vb{V}^{\vb{A}}$ is empty)
                \begin{equation}
                    \vb{V}^{\vb{A}}_1 = \begin{pmatrix} \vb{v}^{\vb{A}}_1 \end{pmatrix} \thinspace .
                \end{equation}

            \item Calculate the Rayleigh quotient:
                \begin{align}
                    \theta &= \vb{v}^{\text{T}}_1 \vb{A} \vb{v}_1 \\
                    &= \vb{v}^{\text{T}}_1 \vb{v}^{\vb{A}}_1 \thinspace .
                \end{align}

            \item Calculate the the associated residual vector:
                \begin{equation}
                    \vb{r}_1 = \vb{v}^{\vb{A}}_1 - \theta \thinspace \vb{v}_1 \thinspace .
                \end{equation}

            \item If the norm of the residual vector is lower than a specified threshold, the algorithm has already converged (but let's not get our hopes up: this doesn't happen) and we have found the lowest eigenpair of $\vb{A}$, being $(\theta, \vb{v}_1)$.

            \item Approximately solve the residue correction equation. This is where the 'Davidson' algorithm got his name. By using a diagonal approximation $\vb{A}'$ of the matrix $\vb{A}$, we get a clear and easy expression for a new $\vb{t}$-vector
                \begin{equation}
                    \vb{t} = (\theta \thinspace \vb{I} - \vb{A}')^{-1} \thinspace \vb{r} \thinspace ,
                \end{equation}
                or, written coefficient-wise:
                \begin{equation}
                    t_i = \frac{r_i}{\theta - A_{ii}} \thinspace .
                \end{equation}
                Davidson originally suggested that if $|\theta - A_{ii}|$ is smaller than a threshold, the components $t_i$ are set to zero.
        \end{enumerate}

        We are now in the position to phrase the algorithm for all iterations $k>1$.

        \begin{enumerate}
            \item Project $\vb{t}$ onto the orthogonal complement of $\vb{V}_{k-1}$:
                \begin{align}
                    \vb{t}^{\perp} &= (\vb{I} - \vb{V}_{k-1} \vb{V}_{k-1}^{\text{T}}) \vb{t} \\
                    &= \vb{t} - \sum_i^{\dim{V_{k-1}}} (\vb{v}_i^{\text{T}} \vb{t}) \vb{v}_i
                \end{align}

            \item Calculate the new subspace vector:
                \begin{equation}
                    \vb{v}_k = \frac{\vb{t}^\perp}{|| \vb{t}^\perp ||} \thinspace .
                \end{equation}

            \item Calculate the (expensive) matrix-vector product:
                \begin{equation}
                    \vb{v}^{\vb{A}}_k = \vb{A} \vb{v}_k \thinspace .
                \end{equation}

            \item Expand the subspace $\vb{V}$ to
                \begin{equation}
                    \vb{V}_k = \begin{pmatrix} \vb{V}_{k-1} & \vb{v}_k \end{pmatrix}
                \end{equation}
                and expand
                \begin{equation}
                    \vb{V}^{\vb{A}}_k = \begin{pmatrix} \vb{V}^{\vb{A}}_{k-1} & \vb{v}_k \end{pmatrix} \thinspace .
                \end{equation}

            \item Calculate the subspace matrix:
                \begin{equation}
                    \vb{M}_k = \vb{V}^{\text{T}}_k \vb{A} \vb{V}_k \thinspace .
                \end{equation}

                Since in iteration $k$, we have already calculated the upper-left $\dim{V_{k-1}} \times \dim{V_{k-1}}$ block of $\vb{M}_k$, we actually only require to calculate
                \begin{equation}
                    M_{ik} = \vb{v}_i^{\text{T}} \vb{v}_k^{\vb{A}} = M_{ki}
                \end{equation}
                for $i = 1, \dots, k$, which is the same as calculating the $k$-th row of $\vb{M}$:
                \begin{align}
                    \vb{m}_k &= \vb{V}^{\text{T}} (\vb{A} \vb{v}_k) \\
                    &= \vb{V}^{\text{T}} \vb{v}^{\vb{A}}_k \thinspace .
                \end{align}

            \item Solve the subspace eigenvalue problem:
                \begin{equation}
                    \vb{M}_k \vb{s} = \theta \thinspace \vb{s} \thinspace .
                \end{equation}

            \item Calculate the new residual vector by first calculating
                \begin{equation}
                    \vb{u} = \vb{V}_k \vb{s}
                \end{equation}
                and
                \begin{equation}
                    \vb{u}^{\vb{A}} = \vb{V}_k^{\vb{A}} \vb{s} \thinspace ,
                \end{equation}
                and subsequently calculation the actual residual vector
                \begin{align}
                    \vb{r} &= (\vb{A} - \theta \thinspace \vb{I}) \vb{V}_k \vb{s} \\
                    &= \vb{u}^{\vb{A}} - \theta \thinspace \vb{u} \thinspace .
                \end{align}

            \item If the norm of the residual vector is lower than a specified threshold, the algorithm has converged and we have found the lowest eigenpair of $\vb{A}$, being $(\theta, \vb{u}_k)$.

            \item Approximately solve the residue correction equation:
                \begin{equation}
                    \vb{t} = (\theta \thinspace \vb{I} - \vb{A}')^{-1} \vb{r} \thinspace ,
                \end{equation}
                or, written coefficient-wise:
                \begin{equation}
                    t_i = \frac{r_i}{\theta - A_{ii}} \thinspace .
                \end{equation}
                Davidson originally suggested that if $|\theta - A_{ii}|$ is smaller than a threshold, the components $t_i$ are set to zero.
        \end{enumerate}

        In case the dimension of the subspace matrix is getting too big (bigger than a predetermined maximum subspace dimension $D$ of $10$-$15$), the algorithm should restart:
        \begin{itemize}
            \item Pulay wrote that taking the two latest ones is enough to prevent convergence instabilities.
            \item We can therefore take 2 linear combinations
                \begin{equation}
                    \vb{v}^{'(j)} = \vb{V}_k \vb{s}^{(j)} \thinspace ,
                \end{equation}
                in which $\vb{V}_k$ is the current subspace, $(j)$ is $1$ or $2$ and $\vb{s}^{(j)}$ is either the lowest or second lowest eigenvector of the subspace matrix $\vb{M}$.
        \end{itemize}
